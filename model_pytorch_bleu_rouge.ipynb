{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6TpL-zc7b1ax",
        "outputId": "e44d09ab-163c-4807-815f-46ce126b498e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.17.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.4.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.2)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.19.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.25.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.23.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.14.0)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "G3Rj16dO2fAs"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "\n",
        "import wandb\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "J7wUHU29dIEJ",
        "outputId": "6d685e20-496a-424d-880a-4299625cf1db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m1631674\u001b[0m (\u001b[33muab-enric\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240605_083950-ia6k9sax</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/uab-enric/uab-grup14/runs/ia6k9sax' target=\"_blank\">prueba rouge</a></strong> to <a href='https://wandb.ai/uab-enric/uab-grup14' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/uab-enric/uab-grup14' target=\"_blank\">https://wandb.ai/uab-enric/uab-grup14</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/uab-enric/uab-grup14/runs/ia6k9sax' target=\"_blank\">https://wandb.ai/uab-enric/uab-grup14/runs/ia6k9sax</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/uab-enric/uab-grup14/runs/ia6k9sax?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7fb5f6cfb340>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "api_key = \"96d6dba1df0340184f3a72f7a53ab98346487632\"\n",
        "wandb.login(key = api_key)\n",
        "wandb.init(project=\"uab-grup14\", name = 'prueba rouge')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DrKeHbM62o4m"
      },
      "outputs": [],
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-WQvjZRA2tE8"
      },
      "outputs": [],
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
        "    return s.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rFnuZPzW2y2q"
      },
      "outputs": [],
      "source": [
        "def readLangs(file, lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open(file, encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZpzpDkmpO864"
      },
      "outputs": [],
      "source": [
        "def eliminar_ccby(linea):\n",
        "    indice_ccby = linea.find(\"CC-BY\")\n",
        "    if indice_ccby != -1:\n",
        "        return linea[:indice_ccby]\n",
        "    return linea\n",
        "\n",
        "def separar_frases(linea):\n",
        "    patron = r'(.+?[.!?])\\s+([^A-Z]?[\\s]+)?(.+)?'\n",
        "    coincidencias = re.match(patron, linea)\n",
        "    if coincidencias:\n",
        "        frase1 = coincidencias.group(1)\n",
        "        frase2 = coincidencias.group(3)\n",
        "        return frase1.strip(), frase2.strip() if frase2 else None\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "def make_dataset(archivo_entrada):\n",
        "    dataset = []\n",
        "    with open('deuOK.txt', 'w') as out:\n",
        "        with open(archivo_entrada, 'r', encoding='utf-8') as f_in:\n",
        "            for linea in f_in:\n",
        "                linea_clean = eliminar_ccby(linea)\n",
        "                a, b = separar_frases(linea_clean)\n",
        "                if a is not None and b is not None:\n",
        "                    out.write(f'{a}\\t{b}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": true,
        "id": "M3PGZlN3PGQI"
      },
      "outputs": [],
      "source": [
        "make_dataset('eng-deu.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6TLxvyjQj2U",
        "outputId": "568af2e0-e9aa-4764-ef31-3a7265ecc4b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n"
          ]
        }
      ],
      "source": [
        "data = readLangs('deuOK.txt','eng','spa')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "FjXOmSZi2zMV"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 5\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dj6cxTPV20-4",
        "outputId": "905f5a50-32fe-4dd5-c3e4-9639e8fa3000"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 158804 sentence pairs\n",
            "Trimmed to 19034 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "deu 6953\n",
            "eng 5052\n",
            "['tom hat entschieden', 'tom decided']\n"
          ]
        }
      ],
      "source": [
        "def prepareData(file, lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(file, lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('deuOK.txt','eng', 'deu', True)\n",
        "print(random.choice(pairs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "sFbA_u1n3f7h"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.gru(embedded)\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "6srjLmKv3ge4"
      },
      "outputs": [],
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "\n",
        "        for i in range(MAX_LENGTH):\n",
        "            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n",
        "            decoder_outputs.append(decoder_output)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n",
        "\n",
        "    def forward_step(self, input, hidden):\n",
        "        output = self.embedding(input)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.out(output)\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Cbeiblfo3io9"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Va = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, query, keys):\n",
        "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
        "        scores = scores.squeeze(2).unsqueeze(1)\n",
        "\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        context = torch.bmm(weights, keys)\n",
        "\n",
        "        return context, weights\n",
        "\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.attention = BahdanauAttention(hidden_size)\n",
        "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "        attentions = []\n",
        "\n",
        "        for i in range(MAX_LENGTH):\n",
        "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            decoder_outputs.append(decoder_output)\n",
        "            attentions.append(attn_weights)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        attentions = torch.cat(attentions, dim=1)\n",
        "\n",
        "        return decoder_outputs, decoder_hidden, attentions\n",
        "\n",
        "\n",
        "    def forward_step(self, input, hidden, encoder_outputs):\n",
        "        embedded =  self.dropout(self.embedding(input))\n",
        "\n",
        "        query = hidden.permute(1, 0, 2)\n",
        "        context, attn_weights = self.attention(query, encoder_outputs)\n",
        "        input_gru = torch.cat((embedded, context), dim=2)\n",
        "\n",
        "        output, hidden = self.gru(input_gru, hidden)\n",
        "        output = self.out(output)\n",
        "\n",
        "        return output, hidden, attn_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "sL92wnT_3kfC"
      },
      "outputs": [],
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)\n",
        "\n",
        "def get_dataloader(batch_size):\n",
        "    input_lang, output_lang, pairs = prepareData('deuOK.txt', 'eng', 'deu', True)\n",
        "\n",
        "    np.random.shuffle(pairs)\n",
        "\n",
        "    n = int(len(pairs) * 0.9)\n",
        "    train_pairs = pairs[:n]\n",
        "    test_pairs = pairs[n:]\n",
        "\n",
        "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
        "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
        "    input_ids2 = np.zeros((len(test_pairs), MAX_LENGTH), dtype=np.int32)\n",
        "    target_ids2 = np.zeros((len(test_pairs), MAX_LENGTH), dtype=np.int32)\n",
        "\n",
        "    for i, pair in enumerate(train_pairs):\n",
        "        inp_ids = indexesFromSentence(input_lang, pair[0])\n",
        "        tgt_ids = indexesFromSentence(output_lang, pair[1])\n",
        "        inp_ids.append(EOS_token)\n",
        "        tgt_ids.append(EOS_token)\n",
        "        input_ids[i, :len(inp_ids)] = inp_ids\n",
        "        target_ids[i, :len(tgt_ids)] = tgt_ids\n",
        "\n",
        "    for i, pair in enumerate(test_pairs):\n",
        "        inp_ids2 = indexesFromSentence(input_lang, pair[0])\n",
        "        tgt_ids2 = indexesFromSentence(output_lang, pair[1])\n",
        "        inp_ids2.append(EOS_token)\n",
        "        tgt_ids2.append(EOS_token)\n",
        "        input_ids2[i, :len(inp_ids2)] = inp_ids2\n",
        "        target_ids2[i, :len(tgt_ids2)] = tgt_ids2\n",
        "\n",
        "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
        "                               torch.LongTensor(target_ids).to(device))\n",
        "    test_data = TensorDataset(torch.LongTensor(input_ids2).to(device),\n",
        "                              torch.LongTensor(target_ids2).to(device))\n",
        "\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "    test_sampler = RandomSampler(test_data)\n",
        "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "    return input_lang, output_lang, train_dataloader, test_dataloader, test_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "tizHqxP53mU5"
      },
      "outputs": [],
      "source": [
        "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
        "          decoder_optimizer, criterion):\n",
        "\n",
        "    total_loss = 0\n",
        "    for data in dataloader:\n",
        "        input_tensor, target_tensor = data\n",
        "\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
        "\n",
        "        loss = criterion(\n",
        "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
        "            target_tensor.view(-1)\n",
        "        )\n",
        "        loss.backward()\n",
        "\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    loss = total_loss / len(dataloader)\n",
        "    wandb.log({'Train Loss':loss, 'Train PPL': np.exp(loss)})\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
        "def evaluate2(encoder, decoder, sentence, input_lang, output_lang):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
        "\n",
        "        _, topi = decoder_outputs.topk(1)\n",
        "        decoded_ids = topi.squeeze()\n",
        "\n",
        "        decoded_words = []\n",
        "        for idx in decoded_ids:\n",
        "            if idx.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            decoded_words.append(output_lang.index2word[idx.item()])\n",
        "    return decoded_words, decoder_attn"
      ],
      "metadata": {
        "id": "0Wpoz-Tx_1Ph"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7V57mjFj5_C",
        "outputId": "9dd3ed57-22a6-4c9a-99d9-72a5e9edd303"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.25.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.4)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=890bf45caa93461577349eede702a80083ba6c7974a48bcadd38d5c5244f9fa2\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "wFRjJ-kgjDDy"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def evaluate_epoch(dataloader, encoder, decoder, criterion, input_lang, output_lang, test_pairs):\n",
        "    total_loss = 0\n",
        "    translations = []\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            input_tensor, target_tensor = data\n",
        "\n",
        "            encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "            decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
        "\n",
        "            loss = criterion(\n",
        "                decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
        "                target_tensor.view(-1)\n",
        "            )\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        for pair in test_pairs:\n",
        "            candidate, _ = evaluate2(encoder, decoder, pair[0], input_lang, output_lang)\n",
        "            translations.append([pair[1]])\n",
        "            predictions.append(' '.join(candidate))\n",
        "\n",
        "    # BLEU\n",
        "    bleu_score = bleu.compute(predictions=predictions, references=translations)\n",
        "    bleu_default = bleu_score['bleu']\n",
        "    bleu_n_grams = bleu_score['precisions']\n",
        "    bleu1, bleu2, bleu3, bleu4 = bleu_n_grams[0], bleu_n_grams[1], bleu_n_grams[2], bleu_n_grams[3]\n",
        "\n",
        "    # ROUGE\n",
        "    rouge_score = rouge.compute(predictions=predictions, references=translations)\n",
        "    rouge1 = rouge_score['rouge1']\n",
        "    rouge2 = rouge_score['rouge2']\n",
        "    rougeL = rouge_score['rougeL']\n",
        "    rougeLsum = rouge_score['rougeLsum']\n",
        "\n",
        "\n",
        "    wandb.log({'Bleu': bleu_default, 'Bleu1': bleu1, 'Bleu2': bleu2, 'Bleu3': bleu3, 'Bleu4': bleu4})\n",
        "    wandb.log({'Rouge1': rouge1, 'Rouge2': rouge2, 'RougeL': rougeL, 'RougeLsum': rougeLsum})\n",
        "    wandb.log({'Valid Loss': total_loss / len(dataloader), 'Valid PPL': np.exp(total_loss / len(dataloader))})\n",
        "\n",
        "    return total_loss / len(dataloader), bleu_score, rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "n5NVH3343oCE"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "zhlqmn_i3poG"
      },
      "outputs": [],
      "source": [
        "def train(train_dataloader, test_dataloader, encoder, decoder, input_lang, output_lang, n_epochs, learning_rate, print_every, plot_every, test_pairs):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print(f'epoch {epoch} train done - loss {loss}')\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        val_loss, bleu, rouge = evaluate_epoch(test_dataloader, encoder, decoder, criterion, input_lang, output_lang, test_pairs)\n",
        "        print(f'epoch {epoch} valid done - loss {val_loss} - bleu {bleu}')\n",
        "        print(f'rouge {rouge}')\n",
        "\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
        "                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n",
        "\n",
        "\n",
        "        if epoch % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    #showPlot(plot_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "cG0Ki6RU3r5l"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "IA6b8Dy53xUB"
      },
      "outputs": [],
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, _ = evaluate2(encoder, decoder, pair[0], input_lang, output_lang)\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "V0HzfW2l3y11",
        "outputId": "79e0ea03-bce1-484a-e3e4-307733bab12e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 158804 sentence pairs\n",
            "Trimmed to 19034 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "deu 6953\n",
            "eng 5052\n"
          ]
        }
      ],
      "source": [
        "hidden_size = 128\n",
        "batch_size = 32\n",
        "\n",
        "input_lang, output_lang, train_dataloader, test_dataloader, test_pairs = get_dataloader(batch_size)\n",
        "\n",
        "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4z7QhJau23Dm",
        "outputId": "d7115ee5-964d-40a1-ebeb-be461174b1f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 train done - loss 2.6579478334135085\n",
            "epoch 1 valid done - loss 2.5836746732393903 - bleu {'bleu': 0.02225535806840334, 'precisions': [0.21265941632110164, 0.06968833318010481, 0.01671681711802073, 0.0009902390720045269], 'brevity_penalty': 1.0, 'length_ratio': 1.9170541472926355, 'translation_length': 12781, 'reference_length': 6667}\n",
            "rouge {'rouge1': 0.33854687708416764, 'rouge2': 0.12467862144858013, 'rougeL': 0.33788953081232537, 'rougeLsum': 0.3377328014539154}\n",
            "epoch 2 train done - loss 2.197256819970572\n",
            "epoch 2 valid done - loss 2.356127504507701 - bleu {'bleu': 0.030889254992723734, 'precisions': [0.23076923076923078, 0.08521439579367238, 0.023885175851868085, 0.0019382528035442337], 'brevity_penalty': 1.0, 'length_ratio': 1.9401529923503824, 'translation_length': 12935, 'reference_length': 6667}\n",
            "rouge {'rouge1': 0.3680017840469524, 'rouge2': 0.15290241096438584, 'rougeL': 0.36704827764439096, 'rougeLsum': 0.36674544817927124}\n",
            "epoch 3 train done - loss 1.8542958973948636\n",
            "epoch 3 valid done - loss 2.1890915711720784 - bleu {'bleu': 0.04639215980692217, 'precisions': [0.24914516630400996, 0.1007843852608537, 0.033002207505518764, 0.005589714924538848], 'brevity_penalty': 1.0, 'length_ratio': 1.9301034948252587, 'translation_length': 12868, 'reference_length': 6667}\n",
            "rouge {'rouge1': 0.39815738795518185, 'rouge2': 0.1812037314925961, 'rougeL': 0.3964969321061752, 'rougeLsum': 0.39613595438175225}\n",
            "epoch 4 train done - loss 1.5710347297031488\n",
            "epoch 4 valid done - loss 2.068687442938487 - bleu {'bleu': 0.057820869074739434, 'precisions': [0.2620996025251344, 0.11238217259998169, 0.0415604566108833, 0.009130495856159573], 'brevity_penalty': 1.0, 'length_ratio': 1.9245537723113844, 'translation_length': 12831, 'reference_length': 6667}\n",
            "rouge {'rouge1': 0.41847009637188104, 'rouge2': 0.20082032813125106, 'rougeL': 0.417342145191409, 'rougeLsum': 0.4169526143790845}\n",
            "epoch 5 train done - loss 1.3301819076956207\n",
            "epoch 5 valid done - loss 1.9655619462331135 - bleu {'bleu': 0.06658982523976804, 'precisions': [0.2707488299531981, 0.1234884573103701, 0.04804704837993786, 0.012239729881823298], 'brevity_penalty': 1.0, 'length_ratio': 1.9229038548072597, 'translation_length': 12820, 'reference_length': 6667}\n",
            "rouge {'rouge1': 0.43245569060957634, 'rouge2': 0.2204444277711064, 'rougeL': 0.43126083766839945, 'rougeLsum': 0.4313571261838059}\n",
            "4m 26s (- 4m 26s) (5 50%) 1.9221\n",
            "epoch 6 train done - loss 1.1234441305051988\n",
            "epoch 6 valid done - loss 1.8864178657531738 - bleu {'bleu': 0.07713176135062125, 'precisions': [0.2817398119122257, 0.13301400147383935, 0.056411974977658624, 0.01674233825198638], 'brevity_penalty': 1.0, 'length_ratio': 1.9139043047847608, 'translation_length': 12760, 'reference_length': 6667}\n",
            "rouge {'rouge1': 0.4509382919834598, 'rouge2': 0.23795268107242679, 'rougeL': 0.44978804021608676, 'rougeLsum': 0.44948291816726604}\n",
            "epoch 7 train done - loss 0.9481212882630861\n",
            "epoch 7 valid done - loss 1.8132837375005086 - bleu {'bleu': 0.08984104267172104, 'precisions': [0.2944496707431797, 0.1462403243641725, 0.06660706303084488, 0.022714366837024418], 'brevity_penalty': 1.0, 'length_ratio': 1.9133043347832608, 'translation_length': 12756, 'reference_length': 6667}\n",
            "rouge {'rouge1': 0.47106967787114806, 'rouge2': 0.2608743497398931, 'rougeL': 0.47016577464319026, 'rougeLsum': 0.47006844404428516}\n",
            "epoch 8 train done - loss 0.8041226522468808\n",
            "epoch 8 valid done - loss 1.7792026142279307 - bleu {'bleu': 0.08993895517852436, 'precisions': [0.3006139798488665, 0.15018518518518517, 0.06710881294964029, 0.021596109839816933], 'brevity_penalty': 1.0, 'length_ratio': 1.9055047247637618, 'translation_length': 12704, 'reference_length': 6667}\n",
            "rouge {'rouge1': 0.48232959850606855, 'rouge2': 0.2695615746298492, 'rougeL': 0.4810205332132848, 'rougeLsum': 0.4811211984793914}\n",
            "epoch 9 train done - loss 0.6789905058581438\n",
            "epoch 9 valid done - loss 1.7532553374767303 - bleu {'bleu': 0.09768344880124313, 'precisions': [0.3067421918023759, 0.15712038493568983, 0.0734583848141076, 0.02571795970852979], 'brevity_penalty': 1.0, 'length_ratio': 1.9065546722663866, 'translation_length': 12711, 'reference_length': 6667}\n",
            "rouge {'rouge1': 0.4916835484193676, 'rouge2': 0.2815026010404137, 'rougeL': 0.4899220104708543, 'rougeLsum': 0.48980967386954744}\n",
            "epoch 10 train done - loss 0.576226281085566\n",
            "epoch 10 valid done - loss 1.7365740478038787 - bleu {'bleu': 0.10303454635151671, 'precisions': [0.31080336769218664, 0.16307265155020823, 0.0774070329176497, 0.02872659711304845], 'brevity_penalty': 1.0, 'length_ratio': 1.9062546872656367, 'translation_length': 12709, 'reference_length': 6667}\n",
            "rouge {'rouge1': 0.4972507753101242, 'rouge2': 0.2906300020007978, 'rougeL': 0.495219962985194, 'rougeLsum': 0.4950253017873819}\n",
            "8m 47s (- 0m 0s) (10 100%) 0.8262\n"
          ]
        }
      ],
      "source": [
        "train(train_dataloader, test_dataloader, encoder, decoder, input_lang, output_lang, 10, 0.001, 5, 5, test_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0_SRM7330Ua",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4beda49-6814-4e95-8bdd-3d4f42440076"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> ich hasse franzosisch\n",
            "= i hate french\n",
            "< i hate french to speak french <EOS>\n",
            "\n",
            "> es funktioniert tatsachlich\n",
            "= it really works\n",
            "< it really worked on <EOS>\n",
            "\n",
            "> tom hat eine waffe\n",
            "= tom has a gun\n",
            "< tom has a gun cry <EOS>\n",
            "\n",
            "> sie ist uberaktiv\n",
            "= she s hyperactive\n",
            "< she is in a born <EOS>\n",
            "\n",
            "> vertrau mir einfach\n",
            "= come on trust me\n",
            "< just follow me just please <EOS>\n",
            "\n",
            "> ich bin hereingelegt worden\n",
            "= i ve been tricked\n",
            "< i ve been tricked <EOS>\n",
            "\n",
            "> tom wandte sich ab\n",
            "= tom glanced away\n",
            "< tom turned down <EOS>\n",
            "\n",
            "> dieser tee gefallt mir\n",
            "= i like this tea\n",
            "< i like that tea <EOS>\n",
            "\n",
            "> tom fing den ball\n",
            "= tom caught the ball\n",
            "< tom caught the ball <EOS>\n",
            "\n",
            "> wir sind die besten\n",
            "= we are the best\n",
            "< we are the best best <EOS>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "encoder.eval()\n",
        "decoder.eval()\n",
        "evaluateRandomly(encoder, decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "EbOKkyy_34Cl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a486883-fe08-47bb-ba47-aa37a7d6e992"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input = das ist dein hund\n",
            "output = this is your dog <EOS>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-39-4441687b87d0>:8: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
            "<ipython-input-39-4441687b87d0>:10: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
            "  ax.set_yticklabels([''] + output_words)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input = zeichne mir ein schaf\n",
            "output = draw me a sheep <EOS>\n"
          ]
        }
      ],
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.cpu().numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
        "                       ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def evaluateAndShowAttention(input_sentence):\n",
        "    output_words, attentions = evaluate2(encoder, decoder, input_sentence, input_lang, output_lang)\n",
        "    print('input =', input_sentence)\n",
        "    print('output =', ' '.join(output_words))\n",
        "    showAttention(input_sentence, output_words, attentions[0, :len(output_words), :])\n",
        "\n",
        "\n",
        "\n",
        "evaluateAndShowAttention('das ist dein hund')\n",
        "\n",
        "evaluateAndShowAttention('zeichne mir ein schaf')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}